import os
from datetime import datetime
import uuid
import pandas as pd
import xml.etree.ElementTree as ET
from flask import Flask, request
from google.cloud import storage
from google.cloud import bigquery
from google.api_core import exceptions

# --- Configura√ß√µes do BigQuery ---
PROJECT_ID = "sud-leather"
DATASET_ID = "RAW_DATA"
TABLE_ID = "Frigorifico_Nota_Fiscal"

# Inicializa os clientes do Cloud Storage e BigQuery
storage_client = storage.Client()
bigquery_client = bigquery.Client(project=PROJECT_ID)

app = Flask(__name__)

def criar_df_nfe(xml_content):
    """
    Processa o conte√∫do de um XML de NFe e retorna um DataFrame do Pandas com os dados dos produtos.
    """
    try:
        namespaces = {'nfe': 'http://www.portalfiscal.inf.br/nfe'}
        root = ET.fromstring(xml_content)
        infNFe = root.find('.//nfe:infNFe', namespaces)

        if infNFe is None:
            print("Tag <infNFe> n√£o encontrada no XML.")
            return None

        # --- Extra√ß√£o de dados do cabe√ßalho da NFe ---
        numero_nf = infNFe.find('.//nfe:ide/nfe:nNF', namespaces).text
        data_emissao_str = (infNFe.find('.//nfe:ide/nfe:dhEmi', namespaces) or infNFe.find('.//nfe:ide/nfe:dEmi', namespaces)).text
        emitente_nome = infNFe.find('.//nfe:emit/nfe:xNome', namespaces).text
        emitente_cnpj = infNFe.find('.//nfe:emit/nfe:CNPJ', namespaces).text
        qvol_element = infNFe.find('.//nfe:transp/nfe:vol/nfe:qVol', namespaces)
        quantidade_pecas = int(float(qvol_element.text)) if qvol_element is not None and qvol_element.text else 0
        
        # Formata a data de emiss√£o
        date_str_sem_fuso = data_emissao_str.split('T')[0]
        data_emissao_formatada = datetime.strptime(date_str_sem_fuso, '%Y-%m-%d').date()

        # --- Extra√ß√£o dos itens da NFe ---
        lista_produtos = []
        for det in infNFe.findall('.//nfe:det', namespaces):
            produto = {
                'numero_nf': int(numero_nf),
                'data_emissao': data_emissao_formatada,
                'emitente': emitente_nome,
                'CNPJ': emitente_cnpj,
                'Descricao': det.find('.//nfe:prod/nfe:xProd', namespaces).text,
                'Quantidade_pcs': quantidade_pecas,
                'Quantidade_kg': float(det.find('.//nfe:prod/nfe:qCom', namespaces).text or '0'),
                'valor_unitario': float(det.find('.//nfe:prod/nfe:vUnCom', namespaces).text or '0'),
                'valor_total_produto': float(det.find('.//nfe:prod/nfe:vProd', namespaces).text or '0')
            }
            lista_produtos.append(produto)

        return pd.DataFrame(lista_produtos)

    except Exception as e:
        print(f"‚ùå Erro ao processar o conte√∫do do XML: {e}")
        return None

def mover_blob_para(bucket, blob, pasta_destino):
    """
    Move um blob para uma pasta de destino (erros ou processados).
    """
    try:
        now = datetime.now()
        destination_folder = f"{pasta_destino}/{now.year:04d}/{now.month:02d}"
        new_path = f"{destination_folder}/{os.path.basename(blob.name)}"
        
        bucket.copy_blob(blob, bucket, new_path)
        blob.delete()
        print(f"‚úÖ Arquivo movido para: {new_path}")
    except Exception as e:
        print(f"üî• Falha cr√≠tica ao mover o arquivo {blob.name} para {pasta_destino}. Erro: {e}")
        # Se falhar ao mover, o arquivo original permanece para nova tentativa.


@app.route("/", methods=["POST"])
def process_nfe_xml():
    """
    Fun√ß√£o principal, acionada por um evento, agora robusta contra loops.
    """
    data = request.get_json(silent=True)
    if not data or "message" not in data:
        print("Requisi√ß√£o inv√°lida, sem payload 'message'.")
        return "Requisi√ß√£o inv√°lida", 400

    message = data["message"]
    attributes = message.get("attributes", {})
    bucket_name = attributes.get("bucketId")
    file_name = attributes.get("objectId")

    if not bucket_name or not file_name or not file_name.lower().endswith(".xml"):
        print(f"üìÅ Arquivo ignorado (n√£o √© XML ou payload inv√°lido): {file_name}")
        # Retorna 200 para n√£o tentar novamente um arquivo que n√£o √© de interesse.
        return "Arquivo ignorado", 200

    print(f"üìÇ Processando arquivo: {file_name} do bucket: {bucket_name}")
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(file_name)

    try:
        # --- MELHORIA 1: Tratamento de arquivo n√£o encontrado ---
        # Tenta baixar o conte√∫do do arquivo. Se n√£o existir, causa uma exce√ß√£o.
        xml_content = blob.download_as_text()

    except exceptions.NotFound:
        # Se o arquivo n√£o for encontrado (erro 404), registramos o problema
        # e retornamos 200 OK para quebrar o loop de uma "mensagem fantasma".
        print(f"‚ùå Erro 404: Arquivo {file_name} n√£o encontrado no bucket. Mensagem descartada.")
        return "Arquivo n√£o encontrado, mensagem descartada", 200

    # Processa o conte√∫do do XML para criar o DataFrame
    df_nfe = criar_df_nfe(xml_content)

    # --- MELHORIA 2: Tratamento de XML inv√°lido ou vazio ---
    if df_nfe is None or df_nfe.empty:
        print(f"‚ö†Ô∏è Nenhum dado extra√≠do de {file_name}. Movendo para a pasta de erros.")
        mover_blob_para(bucket, blob, "erros")
        # Retorna 200 OK para quebrar o loop ap√≥s mover o arquivo inv√°lido.
        return "Arquivo com dados inv√°lidos ou vazios", 200

    # --- L√ìGICA DE MERGE (mantida como estava) ---
    temp_table_id = f"temp_nfe_{uuid.uuid4().hex}"
    temp_table_ref = bigquery_client.dataset(DATASET_ID).table(temp_table_id)

    try:
        job_config = bigquery.LoadJobConfig(autodetect=True, write_disposition="WRITE_TRUNCATE")
        bigquery_client.load_table_from_dataframe(df_nfe, temp_table_ref, job_config=job_config).result()
        print(f"Dados carregados na tabela tempor√°ria: {temp_table_id}")

        merge_query = f"""
            MERGE `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}` AS T
            USING `{PROJECT_ID}.{DATASET_ID}.{temp_table_id}` AS S
            ON T.CNPJ = S.CNPJ AND T.numero_nf = S.numero_nf AND T.Descricao = S.Descricao
            WHEN NOT MATCHED THEN
              INSERT (numero_nf, data_emissao, emitente, CNPJ, Descricao, Quantidade_pcs, Quantidade_kg, valor_unitario, valor_total_produto)
              VALUES(S.numero_nf, S.data_emissao, S.emitente, S.CNPJ, S.Descricao, S.Quantidade_pcs, S.Quantidade_kg, S.valor_unitario, S.valor_total_produto);
        """
        
        print("Executando query MERGE...")
        merge_job = bigquery_client.query(merge_query)
        merge_job.result()

        if merge_job.errors:
            print(f"‚ùå Erros ao executar MERGE: {merge_job.errors}")
            mover_blob_para(bucket, blob, "erros")
            return "Erro ao executar MERGE", 200 # Retorna 200 para evitar loop
        else:
            print(f"‚úÖ MERGE conclu√≠do com sucesso para o arquivo {file_name}.")
            mover_blob_para(bucket, blob, "processados")
            return f"Processado: {file_name}", 200

    except Exception as e:
        print(f"üî• Erro cr√≠tico durante o processo de BigQuery: {str(e)}")
        mover_blob_para(bucket, blob, "erros")
        return f"Erro interno no BigQuery: {str(e)}", 200 # Retorna 200 para evitar loop

    finally:
        bigquery_client.delete_table(temp_table_ref, not_found_ok=True)
        print(f"Tabela tempor√°ria {temp_table_id} apagada.")


if __name__ == "__main__":
    port = int(os.environ.get('PORT', 8080))
    app.run(host='0.0.0.0', port=port)
