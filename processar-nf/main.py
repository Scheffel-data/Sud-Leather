import os
from datetime import datetime
import uuid
import pandas as pd
import xml.etree.ElementTree as ET
from flask import Flask, request
from google.cloud import storage
from google.cloud import bigquery
from google.api_core import exceptions

# --- Configura√ß√µes do BigQuery ---
PROJECT_ID = "sud-leather"
DATASET_ID = "RAW_DATA"
TABLE_ID = "Frigorifico_Nota_Fiscal"

# Inicializa os clientes do Cloud Storage e BigQuery
storage_client = storage.Client()
bigquery_client = bigquery.Client(project=PROJECT_ID)

app = Flask(__name__)

def criar_df_nfe(xml_content):
    """
    Processa o conte√∫do de um XML de NFe e retorna um DataFrame do Pandas com os dados dos produtos.
    """
    try:
        namespaces = {'nfe': 'http://www.portalfiscal.inf.br/nfe'}
        root = ET.fromstring(xml_content)
        infNFe = root.find('.//nfe:infNFe', namespaces)

        if infNFe is None:
            print("Tag <infNFe> n√£o encontrada no XML.")
            return None

        # --- Extra√ß√£o de dados do cabe√ßalho da NFe ---
        numero_nf = infNFe.find('.//nfe:ide/nfe:nNF', namespaces).text
        data_emissao_str = (infNFe.find('.//nfe:ide/nfe:dhEmi', namespaces) or infNFe.find('.//nfe:ide/nfe:dEmi', namespaces)).text
        emitente_nome = infNFe.find('.//nfe:emit/nfe:xNome', namespaces).text
        emitente_cnpj = infNFe.find('.//nfe:emit/nfe:CNPJ', namespaces).text
        qvol_element = infNFe.find('.//nfe:transp/nfe:vol/nfe:qVol', namespaces)
        quantidade_pecas = int(float(qvol_element.text)) if qvol_element is not None and qvol_element.text else 0
        
        # Formata a data de emiss√£o
        date_str_sem_fuso = data_emissao_str.split('T')[0]
        data_emissao_formatada = datetime.strptime(date_str_sem_fuso, '%Y-%m-%d').date()

        # --- Extra√ß√£o dos itens da NFe ---
        lista_produtos = []
        for det in infNFe.findall('.//nfe:det', namespaces):
            produto = {
                'numero_nf': int(numero_nf),
                'data_emissao': data_emissao_formatada,
                'emitente': emitente_nome,
                'CNPJ': emitente_cnpj,
                'Descricao': det.find('.//nfe:prod/nfe:xProd', namespaces).text,
                'Quantidade_pcs': quantidade_pecas,
                'Quantidade_kg': float(det.find('.//nfe:prod/nfe:qCom', namespaces).text or '0'),
                'valor_unitario': float(det.find('.//nfe:prod/nfe:vUnCom', namespaces).text or '0'),
                'valor_total_produto': float(det.find('.//nfe:prod/nfe:vProd', namespaces).text or '0')
            }
            lista_produtos.append(produto)

        return pd.DataFrame(lista_produtos)

    except Exception as e:
        print(f"‚ùå Erro ao processar o conte√∫do do XML: {e}")
        return None

def mover_blob_para(bucket, blob, pasta_destino):
    """
    Move um blob para uma pasta de destino (erros ou processados).
    """
    try:
        now = datetime.now()
        destination_folder = f"{pasta_destino}/{now.year:04d}/{now.month:02d}"
        new_path = f"{destination_folder}/{os.path.basename(blob.name)}"
        
        bucket.copy_blob(blob, bucket, new_path)
        blob.delete()
        print(f"‚úÖ Arquivo movido para: {new_path}")
    except Exception as e:
        print(f"üî• Falha cr√≠tica ao mover o arquivo {blob.name} para {pasta_destino}. Erro: {e}")

@app.route("/", methods=["POST"])
def process_nfe_xml():
    """
    Fun√ß√£o principal, agora adaptada para o formato de payload do Eventarc (CloudEvents).
    """
    # O Eventarc envia um payload JSON que representa um CloudEvent.
    event = request.get_json(silent=True)
    if not event:
        print("Requisi√ß√£o inv√°lida, sem payload JSON.")
        return "Requisi√ß√£o inv√°lida", 400

    # Os dados espec√≠ficos do evento (como nome do bucket e arquivo) est√£o no campo 'data'.
    data = event.get('data', {})
    bucket_name = data.get('bucket')
    file_name = data.get('name') # No CloudEvents, o caminho do arquivo est√° em 'name'.

        # --- Filtro de pasta implementado diretamente no c√≥digo ---
    if not file_name or 'recebidas/' not in file_name:
        print(f"üìÅ Arquivo ignorado (fora da pasta 'recebidas/'): {file_name}")
        return "Arquivo ignorado", 200
    
    # Valida√ß√£o para garantir que temos as informa√ß√µes necess√°rias do evento
    if not bucket_name:
        print(f"‚ùå Erro no payload do evento: 'bucket' n√£o encontrado.")
        return "Payload do evento inv√°lido", 400

    print(f"üìÇ Processando arquivo: {file_name} do bucket: {bucket_name}")
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(file_name)

    try:
        xml_content = blob.download_as_text()
    except exceptions.NotFound:
        print(f"‚ùå Erro 404: Arquivo {file_name} n√£o encontrado no bucket. Mensagem descartada.")
        return "Arquivo n√£o encontrado, mensagem descartada", 200

    df_nfe = criar_df_nfe(xml_content)

    if df_nfe is None or df_nfe.empty:
        print(f"‚ö†Ô∏è Nenhum dado extra√≠do de {file_name}. Movendo para a pasta de erros.")
        mover_blob_para(bucket, blob, "erros")
        return "Arquivo com dados inv√°lidos ou vazios", 200

    temp_table_id = f"temp_nfe_{uuid.uuid4().hex}"
    temp_table_ref = bigquery_client.dataset(DATASET_ID).table(temp_table_id)

    try:
        job_config = bigquery.LoadJobConfig(autodetect=True, write_disposition="WRITE_TRUNCATE")
        bigquery_client.load_table_from_dataframe(df_nfe, temp_table_ref, job_config=job_config).result()
        print(f"Dados carregados na tabela tempor√°ria: {temp_table_id}")

        merge_query = f"""
            MERGE `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}` AS T
            USING `{PROJECT_ID}.{DATASET_ID}.{temp_table_id}` AS S
            ON T.CNPJ = S.CNPJ AND T.numero_nf = S.numero_nf AND T.Descricao = S.Descricao
            WHEN NOT MATCHED THEN
              INSERT (numero_nf, data_emissao, emitente, CNPJ, Descricao, Quantidade_pcs, Quantidade_kg, valor_unitario, valor_total_produto)
              VALUES(S.numero_nf, S.data_emissao, S.emitente, S.CNPJ, S.Descricao, S.Quantidade_pcs, S.Quantidade_kg, S.valor_unitario, S.valor_total_produto);
        """
        
        merge_job = bigquery_client.query(merge_query)
        merge_job.result()

        if merge_job.errors:
            print(f"‚ùå Erros ao executar MERGE: {merge_job.errors}")
            mover_blob_para(bucket, blob, "erros")
        else:
            print(f"‚úÖ MERGE conclu√≠do com sucesso para o arquivo {file_name}.")
            mover_blob_para(bucket, blob, "processados")
        
        return "Processamento finalizado", 200

    except Exception as e:
        print(f"üî• Erro cr√≠tico durante o processo de BigQuery: {str(e)}")
        mover_blob_para(bucket, blob, "erros")
        return f"Erro interno no BigQuery: {str(e)}", 200

    finally:
        bigquery_client.delete_table(temp_table_ref, not_found_ok=True)
        print(f"Tabela tempor√°ria {temp_table_id} apagada.")

if __name__ == "__main__":
    port = int(os.environ.get('PORT', 8080))
    app.run(host='0.0.0.0', port=port)
